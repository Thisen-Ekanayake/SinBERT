{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efedae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_TXT = \"combined.txt\"\n",
    "SHARD_DIR = \"text_shards\"\n",
    "NUM_SHARDS = 100\n",
    "\n",
    "os.makedirs(SHARD_DIR, exist_ok=True)\n",
    "\n",
    "# Count lines (single pass)\n",
    "print(\"Counting lines...\")\n",
    "with open(INPUT_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "lines_per_shard = total_lines // NUM_SHARDS\n",
    "print(f\"Total lines: {total_lines:,}\")\n",
    "print(f\"Lines per shard: {lines_per_shard:,}\")\n",
    "\n",
    "writers = []\n",
    "for i in range(NUM_SHARDS):\n",
    "    f = open(f\"{SHARD_DIR}/shard_{i}.txt\", \"w\", encoding=\"utf-8\")\n",
    "    writers.append(f)\n",
    "\n",
    "with open(INPUT_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total_lines)):\n",
    "        shard_id = min(i // lines_per_shard, NUM_SHARDS - 1)\n",
    "        writers[shard_id].write(line)\n",
    "\n",
    "for f in writers:\n",
    "    f.close()\n",
    "\n",
    "print(\"âœ“ Text sharding complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "TOKENIZER_MODEL = \"tokenizer/unigram_32000_0.9995.model\"\n",
    "TEXT_SHARD_DIR = \"text_shards\"\n",
    "OUT_DIR = \"tokenized_chunks\"\n",
    "\n",
    "MAX_LEN = 256\n",
    "STRIDE = 128\n",
    "CHUNK_SIZE = 10_000   # windows per file\n",
    "PARALLEL_SHARDS = 4   # VERY IMPORTANT\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def tokenize_text_shard(shard_id):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(TOKENIZER_MODEL)\n",
    "\n",
    "    PAD = sp.pad_id()\n",
    "    EOS = sp.eos_id()\n",
    "\n",
    "    buffer = []\n",
    "    chunk = []\n",
    "    chunk_id = 0\n",
    "\n",
    "    in_path = f\"{TEXT_SHARD_DIR}/shard_{shard_id}.txt\"\n",
    "\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Shard {shard_id}\"):\n",
    "            tokens = sp.encode(line.strip(), out_type=int)\n",
    "            tokens.append(EOS)\n",
    "            buffer.extend(tokens)\n",
    "\n",
    "            while len(buffer) >= MAX_LEN:\n",
    "                window = buffer[:MAX_LEN]\n",
    "                buffer = buffer[STRIDE:]\n",
    "\n",
    "                chunk.append({\n",
    "                    \"input_ids\": torch.tensor(window, dtype=torch.long),\n",
    "                    \"attention_mask\": torch.ones(MAX_LEN, dtype=torch.long)\n",
    "                })\n",
    "\n",
    "                if len(chunk) == CHUNK_SIZE:\n",
    "                    out = f\"{OUT_DIR}/s{shard_id}_c{chunk_id}.pt\"\n",
    "                    torch.save(chunk, out)\n",
    "                    chunk.clear()\n",
    "                    chunk_id += 1\n",
    "\n",
    "    # flush remainder\n",
    "    if buffer:\n",
    "        pad = MAX_LEN - len(buffer)\n",
    "        chunk.append({\n",
    "            \"input_ids\": torch.tensor(buffer + [PAD]*pad),\n",
    "            \"attention_mask\": torch.tensor([1]*len(buffer) + [0]*pad)\n",
    "        })\n",
    "\n",
    "    if chunk:\n",
    "        out = f\"{OUT_DIR}/s{shard_id}_c{chunk_id}.pt\"\n",
    "        torch.save(chunk, out)\n",
    "\n",
    "    return shard_id\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    shard_ids = list(range(100))\n",
    "\n",
    "    for i in range(0, 100, PARALLEL_SHARDS):\n",
    "        batch = shard_ids[i:i+PARALLEL_SHARDS]\n",
    "        with Pool(PARALLEL_SHARDS) as p:\n",
    "            p.map(tokenize_text_shard, batch)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
